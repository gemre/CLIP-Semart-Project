{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c9dc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9486da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee6a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -q safetensors\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# 1) Load model & processor\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# Force safetensors to avoid torch.load() (and the torch>=2.6 restriction / CVE gate)\n",
    "model = CLIPModel.from_pretrained(model_id, use_safetensors=True)\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "\n",
    "# 2) Choose device: GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # Print GPU name and count if on CUDA\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# 3) Move model to that device\n",
    "model.to(device)\n",
    "\n",
    "# 4) Load and preprocess an image\n",
    "image_path = \"Images\\\\18759-guard301.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# texts = [\"cat\", \"dog\", \"car\", \"fruit bowl\", \"city\", \"flower\"]\n",
    "texts = [\"Fransesco Guardi\", \"Picasso\", \"Van Gogh\", \"Leonardo da Vinci\",\n",
    "         \"Claude Monet\", \"Salvador Dali\", \"Rembrandt\", \"Michelangelo\", \"Raphael\",\n",
    "         \"Caravaggio\", \"Titian\", \"El Greco\", \"Goya\", \"Vermeer\", \"Botticelli\"]\n",
    "\n",
    "# 5) Preprocess inputs (still on CPU now)\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# 6) Move all input tensors to the same device as the model\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# 7) Run the model\n",
    "outputs = model(**inputs)\n",
    "image_embeddings = outputs.image_embeds\n",
    "text_embeddings  = outputs.text_embeds\n",
    "\n",
    "# 8) Compute similarity\n",
    "import torch.nn.functional as F\n",
    "similarity_scores = F.cosine_similarity(image_embeddings, text_embeddings)\n",
    "\n",
    "print(\"Similarity scores:\", similarity_scores)\n",
    "\n",
    "best_idx = similarity_scores.argmax().item()\n",
    "print(f\"Best match: '{texts[best_idx]}' with score {similarity_scores[best_idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0281ed",
   "metadata": {},
   "source": [
    "## Analysis: Authors with Paintings from Different Time Frames\n",
    "\n",
    "Let's check if the same author has paintings made in different time frames in the semart_train.csv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba13eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the semart_train.csv file\n",
    "df_train = pd.read_csv(\"semart_full_cleaned.csv\")\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(f\"Total number of paintings: {len(df_train)}\")\n",
    "print(f\"\\nColumn names: {df_train.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7980d5e",
   "metadata": {},
   "source": [
    "## Author Prediction Using CLIP\n",
    "\n",
    "Using the same CLIP similarity logic, let's predict the author of 10 random paintings by comparing each image against all authors in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe88b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique authors from the dataset\n",
    "all_authors = df_train['AUTHOR'].unique().tolist()\n",
    "print(f\"Total number of unique authors in dataset: {len(all_authors)}\")\n",
    "\n",
    "# Select 10 random paintings from the dataset\n",
    "import random\n",
    "random.seed(42)  # For reproducibility\n",
    "sample_paintings = df_train.sample(n=10, random_state=42)\n",
    "\n",
    "print(f\"\\nSelected {len(sample_paintings)} random paintings for prediction:\")\n",
    "sample_paintings[['IMAGE_FILE', 'AUTHOR', 'TITLE', 'DATE']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73322b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict author for a given image\n",
    "def predict_author_for_image(image_path, authors_list, model, processor, device):\n",
    "    \"\"\"\n",
    "    Predict the author of a painting using CLIP similarity\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Preprocess inputs\n",
    "        inputs = processor(text=authors_list, images=image, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Move all input tensors to the same device as the model\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Run the model\n",
    "        outputs = model(**inputs)\n",
    "        image_embeddings = outputs.image_embeds\n",
    "        text_embeddings = outputs.text_embeds\n",
    "        \n",
    "        # Compute similarity\n",
    "        similarity_scores = F.cosine_similarity(image_embeddings, text_embeddings)\n",
    "        \n",
    "        # Get best match\n",
    "        best_idx = similarity_scores.argmax().item()\n",
    "        best_author = authors_list[best_idx]\n",
    "        best_score = similarity_scores[best_idx].item()\n",
    "        \n",
    "        # Get top 5 matches\n",
    "        top5_indices = similarity_scores.argsort(descending=True)[:5]\n",
    "        top5_matches = [(authors_list[idx], similarity_scores[idx].item()) for idx in top5_indices]\n",
    "        \n",
    "        return best_author, best_score, top5_matches\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "print(\"Function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1360eacf",
   "metadata": {},
   "source": [
    "## Author Prediction for Top 100 Most Prolific Artists\n",
    "\n",
    "Let's improve prediction accuracy by focusing on the 100 authors with the most paintings in the dataset (excluding Unknown artists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af24c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 100 authors with the most paintings (excluding Unknown artists)\n",
    "author_counts = df_train['AUTHOR'].value_counts()\n",
    "\n",
    "# Filter out authors with \"Unknown\" in their name\n",
    "author_counts_filtered = author_counts[~author_counts.index.str.contains('Unknown', case=False, na=False)]\n",
    "top_100_authors = author_counts_filtered.head(100).index.tolist()\n",
    "\n",
    "print(f\"Top 100 authors with most paintings (excluding Unknown artists):\")\n",
    "print(\"=\"*80)\n",
    "for i, author in enumerate(top_100_authors, 1):\n",
    "    count = author_counts[author]\n",
    "    print(f\"{i:3d}. {author:40s} - {count:3d} paintings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf18c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ALL paintings from the top 100 authors\n",
    "paintings_top100 = df_train[df_train['AUTHOR'].isin(top_100_authors)]\n",
    "\n",
    "print(f\"Total paintings by top 100 authors: {len(paintings_top100)}\")\n",
    "print(f\"\\nWill predict authors for all {len(paintings_top100)} paintings\")\n",
    "print(f\"\\nFirst 10 paintings:\")\n",
    "paintings_top100[['IMAGE_FILE', 'AUTHOR', 'TITLE', 'DATE']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict authors for ALL paintings by top 100 authors (using only top 100 authors as candidates)\n",
    "top100_results = []\n",
    "\n",
    "print(f\"Predicting authors for all {len(paintings_top100)} paintings (Top 100 authors only)...\\n\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "total_paintings = len(paintings_top100)\n",
    "for counter, (idx, row) in enumerate(paintings_top100.iterrows(), 1):\n",
    "    \n",
    "    image_file = row['IMAGE_FILE']\n",
    "    true_author = row['AUTHOR']\n",
    "    title = row['TITLE']\n",
    "    image_path = os.path.join(\"Images\", image_file)\n",
    "    \n",
    "    \n",
    "    # Show progress every 100 paintings or for first 10\n",
    "    if counter <= 10 or counter % 100 == 0:\n",
    "        print(f\"\\n[{counter}/{total_paintings}] Processing: {title} by {true_author}\")\n",
    "    \n",
    "    # Predict author using only top 100 authors\n",
    "    predicted_author, confidence, top5_matches = predict_author_for_image(\n",
    "        image_path, top_100_authors, model, processor, device\n",
    "    )\n",
    "    \n",
    "    if predicted_author:\n",
    "        is_correct = predicted_author == true_author\n",
    "        symbol = \"✅\" if is_correct else \"❌\"\n",
    "        \n",
    "        print(f\"   {symbol} Predicted Author: {predicted_author} (confidence: {confidence:.4f})\")\n",
    "        # Only show detailed predictions for first 10 or every 100th painting\n",
    "        if counter <= 10 or counter % 100 == 0:\n",
    "            print(f\"   {symbol} Predicted: {predicted_author} (confidence: {confidence:.4f})\")\n",
    "            if not is_correct:\n",
    "                print(f\"      Top 5: {', '.join([f'{a} ({s:.3f})' for a, s in top5_matches[:5]])}\")\n",
    "        top100_results.append({\n",
    "            'image_file': image_file,\n",
    "            'title': title,\n",
    "            'true_author': true_author,\n",
    "            'predicted_author': predicted_author,\n",
    "            'confidence': confidence,\n",
    "            'correct': is_correct\n",
    "        })\n",
    "    \n",
    "\n",
    "print(f\"\\n\\nPrediction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857bb562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of results for top 100 authors\n",
    "if top100_results:\n",
    "    \n",
    "    top100_results_df = pd.DataFrame(top100_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"SUMMARY OF PREDICTIONS (TOP 100 AUTHORS)\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Basic accuracy\n",
    "    accuracy = top100_results_df['correct'].sum() / len(top100_results_df) * 100\n",
    "    print(f\"\\nTotal paintings processed: {len(top100_results_df)}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}% ({top100_results_df['correct'].sum()}/{len(top100_results_df)} correct predictions)\")\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    y_true = top100_results_df['true_author']\n",
    "    y_pred = top100_results_df['predicted_author']\n",
    "    \n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nEvaluation Metrics:\")\n",
    "    print(f\"  Macro Average:\")\n",
    "    print(f\"    Precision: {precision_macro:.4f}\")\n",
    "    print(f\"    Recall:    {recall_macro:.4f}\")\n",
    "    print(f\"    F1 Score:  {f1_macro:.4f}\")\n",
    "    print(f\"  Weighted Average:\")\n",
    "    print(f\"    Precision: {precision_weighted:.4f}\")\n",
    "    print(f\"    Recall:    {recall_weighted:.4f}\")\n",
    "    print(f\"    F1 Score:  {f1_weighted:.4f}\")\n",
    "    \n",
    "    print(f\"\\nConfidence Statistics:\")\n",
    "    print(f\"  Average: {top100_results_df['confidence'].mean():.4f}\")\n",
    "    print(f\"  Median: {top100_results_df['confidence'].median():.4f}\")\n",
    "    print(f\"  Max: {top100_results_df['confidence'].max():.4f}\")\n",
    "    print(f\"  Min: {top100_results_df['confidence'].min():.4f}\")\n",
    "    \n",
    "    # Accuracy by confidence level\n",
    "    correct_df = top100_results_df[top100_results_df['correct']]\n",
    "    incorrect_df = top100_results_df[~top100_results_df['correct']]\n",
    "    \n",
    "    print(f\"\\nCorrect predictions by confidence:\")\n",
    "    print(f\"  Correct predictions avg confidence: {correct_df['confidence'].mean():.4f}\")\n",
    "    print(f\"  Incorrect predictions avg confidence: {incorrect_df['confidence'].mean():.4f}\")\n",
    "    \n",
    "    # Per-author accuracy (for authors with at least 10 paintings)\n",
    "    print(f\"\\nPer-Author Performance (authors with 10+ paintings):\")\n",
    "    author_stats = []\n",
    "    for author in top100_results_df['true_author'].unique():\n",
    "        author_data = top100_results_df[top100_results_df['true_author'] == author]\n",
    "        if len(author_data) >= 10:\n",
    "            author_accuracy = (author_data['correct'].sum() / len(author_data)) * 100\n",
    "            author_stats.append({\n",
    "                'author': author,\n",
    "                'total': len(author_data),\n",
    "                'correct': author_data['correct'].sum(),\n",
    "                'accuracy': author_accuracy\n",
    "            })\n",
    "    \n",
    "    author_stats_df = pd.DataFrame(author_stats).sort_values('accuracy', ascending=False)\n",
    "    print(f\"\\nTop 10 Best Performing Authors:\")\n",
    "    for idx, row in author_stats_df.head(10).iterrows():\n",
    "        print(f\"  {row['author']:40s} - {row['accuracy']:.1f}% ({row['correct']}/{row['total']})\")\n",
    "    \n",
    "    print(f\"\\nBottom 10 Authors:\")\n",
    "    for idx, row in author_stats_df.tail(10).iterrows():\n",
    "        print(f\"  {row['author']:40s} - {row['accuracy']:.1f}% ({row['correct']}/{row['total']})\")\n",
    "    \n",
    "    # Most commonly confused authors\n",
    "    print(f\"\\nMost Common Prediction Errors (Top 10):\")\n",
    "    errors = top100_results_df[~top100_results_df['correct']]\n",
    "    \n",
    "    if len(errors) > 0:\n",
    "        error_pairs = errors.groupby(['true_author', 'predicted_author']).size().sort_values(ascending=False).head(10)\n",
    "        for (true_a, pred_a), count in error_pairs.items():\n",
    "            print(f\"  {true_a} → {pred_a}: {count} times\")\n",
    "    \n",
    "    print(\"\\n\\nDetailed Results (first 50 rows):\")\n",
    "    display(top100_results_df[['title', 'true_author', 'predicted_author', 'confidence', 'correct']].head(50))\n",
    "    \n",
    "    print(\"\\nLast 20 rows:\")\n",
    "    display(top100_results_df[['title', 'true_author', 'predicted_author', 'confidence', 'correct']].tail(20))\n",
    "else:\n",
    "    print(\"No results to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_results_df.to_csv(\"top100_results_df.csv\", index=False)\n",
    "author_stats_df.to_csv(\"author_stats_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f772cff",
   "metadata": {},
   "source": [
    "## Apply Grayscale Transformation Using PyTorch\n",
    "\n",
    "Let's select a random painting and apply grayscale transformation using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Select a random painting from the dataset\n",
    "random_painting = df_train.sample(n=1, random_state=42)\n",
    "image_file = random_painting['IMAGE_FILE'].values[0]\n",
    "title = random_painting['TITLE'].values[0]\n",
    "author = random_painting['AUTHOR'].values[0]\n",
    "image_path = os.path.join(\"Images\", image_file)\n",
    "\n",
    "print(f\"Selected painting: {title}\")\n",
    "print(f\"Author: {author}\")\n",
    "print(f\"File: {image_file}\")\n",
    "\n",
    "# Load the image\n",
    "original_image = Image.open(image_path)\n",
    "\n",
    "# Convert PIL Image to PyTorch tensor\n",
    "to_tensor = transforms.ToTensor()\n",
    "image_tensor = to_tensor(original_image)\n",
    "\n",
    "print(f\"\\nOriginal image shape: {image_tensor.shape}\")  # Should be [C, H, W]\n",
    "print(f\"Image size: {original_image.size}\")\n",
    "\n",
    "# Apply grayscale transformation using PyTorch\n",
    "# Method 1: Using torchvision transforms\n",
    "grayscale_transform = transforms.Grayscale(num_output_channels=1)\n",
    "grayscale_tensor = grayscale_transform(original_image)\n",
    "grayscale_tensor = to_tensor(grayscale_tensor)\n",
    "\n",
    "print(f\"Grayscale tensor shape: {grayscale_tensor.shape}\")  # Should be [1, H, W]\n",
    "\n",
    "# Convert tensors back to images for display\n",
    "to_pil = transforms.ToPILImage()\n",
    "grayscale_image = to_pil(grayscale_tensor)\n",
    "\n",
    "# Display original and grayscale images side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "axes[0].imshow(original_image)\n",
    "axes[0].set_title(f'Original\\n{title}\\nby {author}', fontsize=10)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(grayscale_image, cmap='gray')\n",
    "axes[1].set_title(f'Grayscale (PyTorch)\\n{title}\\nby {author}', fontsize=10)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGrayscale transformation applied successfully using PyTorch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de7eea",
   "metadata": {},
   "source": [
    "## Re-predict Authors on Grayscale Images\n",
    "\n",
    "Let's take all the paintings where the author was correctly predicted and apply grayscale transformation to them, then re-predict to see how grayscale affects prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d071adeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict author for a grayscale image\n",
    "def predict_author_for_grayscale_image(image_path, authors_list, model, processor, device):\n",
    "    \"\"\"\n",
    "    Apply grayscale transformation and predict the author of a painting using CLIP similarity\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Apply grayscale transformation\n",
    "        grayscale_transform = transforms.Grayscale(num_output_channels=3)  # Convert to 3 channels for CLIP\n",
    "        grayscale_image = grayscale_transform(image)\n",
    "        \n",
    "        # Preprocess inputs\n",
    "        inputs = processor(text=authors_list, images=grayscale_image, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Move all input tensors to the same device as the model\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Run the model\n",
    "        outputs = model(**inputs)\n",
    "        image_embeddings = outputs.image_embeds\n",
    "        text_embeddings = outputs.text_embeds\n",
    "        \n",
    "        # Compute similarity\n",
    "        similarity_scores = F.cosine_similarity(image_embeddings, text_embeddings)\n",
    "        \n",
    "        # Get best match\n",
    "        best_idx = similarity_scores.argmax().item()\n",
    "        best_author = authors_list[best_idx]\n",
    "        best_score = similarity_scores[best_idx].item()\n",
    "        \n",
    "        return best_author, best_score\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "print(\"Grayscale prediction function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c4dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for correctly predicted paintings\n",
    "correct_predictions = [result for result in top100_results if result['correct']]\n",
    "\n",
    "print(f\"Total correct predictions (original color images): {len(correct_predictions)}\")\n",
    "print(f\"We will apply grayscale to these {len(correct_predictions)} images and re-predict\")\n",
    "\n",
    "# Show sample of correctly predicted paintings\n",
    "print(\"\\nSample of correctly predicted paintings:\")\n",
    "for i, result in enumerate(correct_predictions[:10], 1):\n",
    "    print(f\"{i}. {result['title']} by {result['true_author']} (confidence: {result['confidence']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c61e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-predict authors on grayscale versions of correctly predicted images\n",
    "grayscale_results = []\n",
    "\n",
    "print(f\"Re-predicting authors on {len(correct_predictions)} grayscale images...\\n\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "total = len(correct_predictions)\n",
    "for counter, result in enumerate(correct_predictions, 1):\n",
    "    \n",
    "    image_file = result['image_file']\n",
    "    true_author = result['true_author']\n",
    "    original_prediction = result['predicted_author']\n",
    "    original_confidence = result['confidence']\n",
    "    title = result['title']\n",
    "    image_path = os.path.join(\"Images\", image_file)\n",
    "    \n",
    "    # Show progress every 200 paintings or for first 10\n",
    "    if counter <= 10 or counter % 200 == 0:\n",
    "        print(f\"\\n[{counter}/{total}] Processing: {title} by {true_author}\")\n",
    "    \n",
    "    # Predict author on grayscale image\n",
    "    predicted_author, confidence = predict_author_for_grayscale_image(\n",
    "        image_path, top_100_authors, model, processor, device\n",
    "    )\n",
    "    \n",
    "    if predicted_author:\n",
    "        is_correct = predicted_author == true_author\n",
    "        symbol = \"✅\" if is_correct else \"❌\"\n",
    "        \n",
    "        # Only show detailed predictions for first 10 or every 200th painting\n",
    "        if counter <= 10 or counter % 200 == 0:\n",
    "            print(f\"   Original (color): ✅ {original_prediction} (confidence: {original_confidence:.4f})\")\n",
    "            print(f\"   Grayscale: {symbol} {predicted_author} (confidence: {confidence:.4f})\")\n",
    "        \n",
    "        grayscale_results.append({\n",
    "            'image_file': image_file,\n",
    "            'title': title,\n",
    "            'true_author': true_author,\n",
    "            'original_prediction': original_prediction,\n",
    "            'original_confidence': original_confidence,\n",
    "            'grayscale_prediction': predicted_author,\n",
    "            'grayscale_confidence': confidence,\n",
    "            'still_correct': is_correct\n",
    "        })\n",
    "\n",
    "print(f\"\\n\\nGrayscale re-prediction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbfbe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison of color vs grayscale predictions\n",
    "if grayscale_results:\n",
    "    grayscale_results_df = pd.DataFrame(grayscale_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"GRAYSCALE vs COLOR PREDICTION COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Calculate accuracy on grayscale\n",
    "    grayscale_accuracy = grayscale_results_df['still_correct'].sum() / len(grayscale_results_df) * 100\n",
    "    \n",
    "    print(f\"\\nOriginal Color Images:\")\n",
    "    print(f\"  Total paintings tested: {len(grayscale_results_df)}\")\n",
    "    print(f\"  All were correctly predicted: 100% (these were the correct predictions from original test)\")\n",
    "    \n",
    "    print(f\"\\nGrayscale Images:\")\n",
    "    print(f\"  Total paintings tested: {len(grayscale_results_df)}\")\n",
    "    print(f\"  Still correctly predicted: {grayscale_results_df['still_correct'].sum()}\")\n",
    "    print(f\"  Accuracy: {grayscale_accuracy:.2f}%\")\n",
    "    \n",
    "    # Calculate drop in accuracy\n",
    "    accuracy_drop = 100 - grayscale_accuracy\n",
    "    print(f\"\\nAccuracy Drop: {accuracy_drop:.2f}%\")\n",
    "    print(f\"  ({grayscale_results_df['still_correct'].sum()}/{len(grayscale_results_df)} paintings remained correct after grayscale)\")\n",
    "    \n",
    "    # Confidence comparison\n",
    "    print(f\"\\nConfidence Statistics:\")\n",
    "    print(f\"  Original (color) - Average: {grayscale_results_df['original_confidence'].mean():.4f}\")\n",
    "    print(f\"  Grayscale - Average: {grayscale_results_df['grayscale_confidence'].mean():.4f}\")\n",
    "    \n",
    "    confidence_diff = grayscale_results_df['grayscale_confidence'].mean() - grayscale_results_df['original_confidence'].mean()\n",
    "    print(f\"  Confidence change: {confidence_diff:+.4f}\")\n",
    "    \n",
    "    # Analyze paintings that became incorrect\n",
    "    became_incorrect = grayscale_results_df[~grayscale_results_df['still_correct']]\n",
    "    \n",
    "    if len(became_incorrect) > 0:\n",
    "        print(f\"\\n\\nPaintings that became INCORRECT after grayscale ({len(became_incorrect)} total):\")\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "        print(f\"\\nMost common new predictions (what model confused them with):\")\n",
    "        new_predictions = became_incorrect['grayscale_prediction'].value_counts().head(10)\n",
    "        for author, count in new_predictions.items():\n",
    "            print(f\"  {author}: {count} paintings\")\n",
    "        \n",
    "        print(f\"\\nAuthors most affected by grayscale (lost correct predictions):\")\n",
    "        affected_authors = became_incorrect['true_author'].value_counts().head(10)\n",
    "        for author, count in affected_authors.items():\n",
    "            total_by_author = len(grayscale_results_df[grayscale_results_df['true_author'] == author])\n",
    "            print(f\"  {author}: {count}/{total_by_author} became incorrect ({count/total_by_author*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nSample of paintings that lost correct prediction:\")\n",
    "        display(became_incorrect[['title', 'true_author', 'original_prediction', 'grayscale_prediction', \n",
    "                                   'original_confidence', 'grayscale_confidence']].head(20))\n",
    "    \n",
    "    # Paintings that remained correct\n",
    "    remained_correct = grayscale_results_df[grayscale_results_df['still_correct']]\n",
    "    \n",
    "    if len(remained_correct) > 0:\n",
    "        print(f\"\\n\\nPaintings that remained CORRECT after grayscale ({len(remained_correct)} total):\")\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "        print(f\"\\nAuthors most robust to grayscale:\")\n",
    "        robust_authors = remained_correct['true_author'].value_counts().head(10)\n",
    "        for author, count in robust_authors.items():\n",
    "            total_by_author = len(grayscale_results_df[grayscale_results_df['true_author'] == author])\n",
    "            print(f\"  {author}: {count}/{total_by_author} remained correct ({count/total_by_author*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nSample of paintings that remained correct:\")\n",
    "        display(remained_correct[['title', 'true_author', 'original_confidence', 'grayscale_confidence']].head(20))\n",
    "    \n",
    "    print(\"\\n\\nAll Results (first 50 rows):\")\n",
    "    display(grayscale_results_df[['title', 'true_author', 'original_prediction', 'grayscale_prediction', \n",
    "                                   'original_confidence', 'grayscale_confidence', 'still_correct']].head(50))\n",
    "\n",
    "else:\n",
    "    print(\"No results to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca92628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# Select a random painting from the dataset\n",
    "random_painting = df_train.sample(n=1, random_state=42)\n",
    "image_file = random_painting['IMAGE_FILE'].values[0]\n",
    "title = random_painting['TITLE'].values[0]\n",
    "author = random_painting['AUTHOR'].values[0]\n",
    "image_path = os.path.join(\"Images\", image_file)\n",
    "\n",
    "print(f\"Selected painting: {title}\")\n",
    "print(f\"Author: {author}\")\n",
    "print(f\"File: {image_file}\")\n",
    "\n",
    "# Load the image (IMPORTANT: convert to RGB)\n",
    "original_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "print(f\"\\nOriginal image size: {original_image.size}\")\n",
    "\n",
    "# Define RandomPerspective transform\n",
    "perspective_transform = v2.RandomPerspective(\n",
    "    distortion_scale=0.6,\n",
    "    p=1.0\n",
    ")\n",
    "\n",
    "# Generate multiple perspective-transformed images\n",
    "num_variants = 4\n",
    "perspective_images = [\n",
    "    perspective_transform(original_image) for _ in range(num_variants)\n",
    "]\n",
    "\n",
    "# Plot original + perspective variants\n",
    "fig, axes = plt.subplots(1, num_variants + 1, figsize=(18, 5))\n",
    "\n",
    "# Original\n",
    "axes[0].imshow(original_image)\n",
    "axes[0].set_title(f\"Original\\n{title}\\nby {author}\", fontsize=10)\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Perspective variants\n",
    "for i, img in enumerate(perspective_images, start=1):\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"RandomPerspective #{i}\", fontsize=10)\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRandomPerspective transformation applied successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd9f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_author_for_perspective_image(\n",
    "    image_path,\n",
    "    authors_list,\n",
    "    model,\n",
    "    processor,\n",
    "    device,\n",
    "    distortion_scale=0.6\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply RandomPerspective transform and predict author using CLIP\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Random perspective transform\n",
    "        perspective_transform = v2.RandomPerspective(\n",
    "            distortion_scale=distortion_scale,\n",
    "            p=1.0\n",
    "        )\n",
    "\n",
    "        transformed_image = perspective_transform(image)\n",
    "\n",
    "        # CLIP preprocessing\n",
    "        inputs = processor(\n",
    "            text=authors_list,\n",
    "            images=transformed_image,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            image_embeds = outputs.image_embeds\n",
    "            text_embeds = outputs.text_embeds\n",
    "\n",
    "            similarity_scores = F.cosine_similarity(image_embeds, text_embeds)\n",
    "\n",
    "        best_idx = similarity_scores.argmax().item()\n",
    "        best_author = authors_list[best_idx]\n",
    "        best_score = similarity_scores[best_idx].item()\n",
    "\n",
    "        return best_author, best_score\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1eb466",
   "metadata": {},
   "outputs": [],
   "source": [
    "perspective_results = []\n",
    "\n",
    "print(f\"Re-predicting authors with RandomPerspective on {len(correct_predictions)} images...\\n\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "total = len(correct_predictions)\n",
    "\n",
    "for counter, result in enumerate(correct_predictions, 1):\n",
    "\n",
    "    image_file = result['image_file']\n",
    "    true_author = result['true_author']\n",
    "    original_prediction = result['predicted_author']\n",
    "    original_confidence = result['confidence']\n",
    "    title = result['title']\n",
    "    image_path = os.path.join(\"Images\", image_file)\n",
    "\n",
    "    if counter <= 10 or counter % 200 == 0:\n",
    "        print(f\"\\n[{counter}/{total}] Processing: {title} by {true_author}\")\n",
    "\n",
    "    predicted_author, confidence = predict_author_for_perspective_image(\n",
    "        image_path,\n",
    "        top_100_authors,\n",
    "        model,\n",
    "        processor,\n",
    "        device,\n",
    "        distortion_scale=0.6\n",
    "    )\n",
    "\n",
    "    if predicted_author:\n",
    "        is_correct = predicted_author == true_author\n",
    "        symbol = \"✅\" if is_correct else \"❌\"\n",
    "\n",
    "        if counter <= 10 or counter % 200 == 0:\n",
    "            print(f\"   Original (color): ✅ {original_prediction} ({original_confidence:.4f})\")\n",
    "            print(f\"   Perspective: {symbol} {predicted_author} ({confidence:.4f})\")\n",
    "\n",
    "        perspective_results.append({\n",
    "            'image_file': image_file,\n",
    "            'title': title,\n",
    "            'true_author': true_author,\n",
    "            'original_prediction': original_prediction,\n",
    "            'original_confidence': original_confidence,\n",
    "            'perspective_prediction': predicted_author,\n",
    "            'perspective_confidence': confidence,\n",
    "            'still_correct': is_correct\n",
    "        })\n",
    "\n",
    "print(\"\\n\\nRandomPerspective re-prediction complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d82aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "perspective_df = pd.DataFrame(perspective_results)\n",
    "\n",
    "perspective_accuracy = (\n",
    "    perspective_df['still_correct'].sum() / len(perspective_df) * 100\n",
    ")\n",
    "\n",
    "print(f\"\\nRandomPerspective Accuracy: {perspective_accuracy:.2f}%\")\n",
    "print(f\"Accuracy Drop: {100 - perspective_accuracy:.2f}%\")\n",
    "\n",
    "print(f\"\\nConfidence:\")\n",
    "print(f\"  Original avg: {perspective_df['original_confidence'].mean():.4f}\")\n",
    "print(f\"  Perspective avg: {perspective_df['perspective_confidence'].mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc87248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison of color vs RandomPerspective predictions\n",
    "if perspective_results:\n",
    "    perspective_results_df = pd.DataFrame(perspective_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"RANDOM PERSPECTIVE vs COLOR PREDICTION COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Calculate accuracy on perspective images\n",
    "    perspective_accuracy = (\n",
    "        perspective_results_df['still_correct'].sum()\n",
    "        / len(perspective_results_df) * 100\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nOriginal Color Images:\")\n",
    "    print(f\"  Total paintings tested: {len(perspective_results_df)}\")\n",
    "    print(f\"  All were correctly predicted: 100% (these were the correct predictions from original test)\")\n",
    "    \n",
    "    print(f\"\\nRandomPerspective Images:\")\n",
    "    print(f\"  Total paintings tested: {len(perspective_results_df)}\")\n",
    "    print(f\"  Still correctly predicted: {perspective_results_df['still_correct'].sum()}\")\n",
    "    print(f\"  Accuracy: {perspective_accuracy:.2f}%\")\n",
    "    \n",
    "    # Calculate drop in accuracy\n",
    "    accuracy_drop = 100 - perspective_accuracy\n",
    "    print(f\"\\nAccuracy Drop: {accuracy_drop:.2f}%\")\n",
    "    print(\n",
    "        f\"  ({perspective_results_df['still_correct'].sum()}\"\n",
    "        f\"/{len(perspective_results_df)} paintings remained correct after RandomPerspective)\"\n",
    "    )\n",
    "    \n",
    "    # Confidence comparison\n",
    "    print(f\"\\nConfidence Statistics:\")\n",
    "    print(f\"  Original (color) - Average: {perspective_results_df['original_confidence'].mean():.4f}\")\n",
    "    print(f\"  RandomPerspective - Average: {perspective_results_df['perspective_confidence'].mean():.4f}\")\n",
    "    \n",
    "    confidence_diff = (\n",
    "        perspective_results_df['perspective_confidence'].mean()\n",
    "        - perspective_results_df['original_confidence'].mean()\n",
    "    )\n",
    "    print(f\"  Confidence change: {confidence_diff:+.4f}\")\n",
    "    \n",
    "    # Analyze paintings that became incorrect\n",
    "    became_incorrect = perspective_results_df[~perspective_results_df['still_correct']]\n",
    "    \n",
    "    if len(became_incorrect) > 0:\n",
    "        print(\n",
    "            f\"\\n\\nPaintings that became INCORRECT after RandomPerspective \"\n",
    "            f\"({len(became_incorrect)} total):\"\n",
    "        )\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "        print(f\"\\nMost common new predictions (what model confused them with):\")\n",
    "        new_predictions = (\n",
    "            became_incorrect['perspective_prediction']\n",
    "            .value_counts()\n",
    "            .head(10)\n",
    "        )\n",
    "        for author, count in new_predictions.items():\n",
    "            print(f\"  {author}: {count} paintings\")\n",
    "        \n",
    "        print(f\"\\nAuthors most affected by RandomPerspective (lost correct predictions):\")\n",
    "        affected_authors = became_incorrect['true_author'].value_counts().head(10)\n",
    "        for author, count in affected_authors.items():\n",
    "            total_by_author = len(\n",
    "                perspective_results_df[\n",
    "                    perspective_results_df['true_author'] == author\n",
    "                ]\n",
    "            )\n",
    "            print(\n",
    "                f\"  {author}: {count}/{total_by_author} became incorrect \"\n",
    "                f\"({count/total_by_author*100:.1f}%)\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\nSample of paintings that lost correct prediction:\")\n",
    "        display(\n",
    "            became_incorrect[\n",
    "                [\n",
    "                    'title',\n",
    "                    'true_author',\n",
    "                    'original_prediction',\n",
    "                    'perspective_prediction',\n",
    "                    'original_confidence',\n",
    "                    'perspective_confidence'\n",
    "                ]\n",
    "            ].head(20)\n",
    "        )\n",
    "    \n",
    "    # Paintings that remained correct\n",
    "    remained_correct = perspective_results_df[perspective_results_df['still_correct']]\n",
    "    \n",
    "    if len(remained_correct) > 0:\n",
    "        print(\n",
    "            f\"\\n\\nPaintings that remained CORRECT after RandomPerspective \"\n",
    "            f\"({len(remained_correct)} total):\"\n",
    "        )\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "        print(f\"\\nAuthors most robust to RandomPerspective:\")\n",
    "        robust_authors = remained_correct['true_author'].value_counts().head(10)\n",
    "        for author, count in robust_authors.items():\n",
    "            total_by_author = len(\n",
    "                perspective_results_df[\n",
    "                    perspective_results_df['true_author'] == author\n",
    "                ]\n",
    "            )\n",
    "            print(\n",
    "                f\"  {author}: {count}/{total_by_author} remained correct \"\n",
    "                f\"({count/total_by_author*100:.1f}%)\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\nSample of paintings that remained correct:\")\n",
    "        display(\n",
    "            remained_correct[\n",
    "                [\n",
    "                    'title',\n",
    "                    'true_author',\n",
    "                    'original_confidence',\n",
    "                    'perspective_confidence'\n",
    "                ]\n",
    "            ].head(20)\n",
    "        )\n",
    "    \n",
    "    print(\"\\n\\nAll Results (first 50 rows):\")\n",
    "    display(\n",
    "        perspective_results_df[\n",
    "            [\n",
    "                'title',\n",
    "                'true_author',\n",
    "                'original_prediction',\n",
    "                'perspective_prediction',\n",
    "                'original_confidence',\n",
    "                'perspective_confidence',\n",
    "                'still_correct'\n",
    "            ]\n",
    "        ].head(50)\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"No results to display.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcbf10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict author for an elastic-transformed image\n",
    "def predict_author_for_elastic_image(image_path, authors_list, model, processor, device, alpha=250.0, sigma=5.0):\n",
    "    \"\"\"\n",
    "    Apply elastic transformation and predict the author of a painting using CLIP similarity\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Apply elastic transformation\n",
    "        elastic_transform = transforms.ElasticTransform(alpha=alpha, sigma=sigma)\n",
    "        elastic_image = elastic_transform(image)\n",
    "        \n",
    "        # Preprocess inputs\n",
    "        inputs = processor(text=authors_list, images=elastic_image, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Move all input tensors to the same device as the model\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Run the model\n",
    "        outputs = model(**inputs)\n",
    "        image_embeddings = outputs.image_embeds\n",
    "        text_embeddings = outputs.text_embeds\n",
    "        \n",
    "        # Compute similarity\n",
    "        similarity_scores = F.cosine_similarity(image_embeddings, text_embeddings)\n",
    "        \n",
    "        # Get best match\n",
    "        best_idx = similarity_scores.argmax().item()\n",
    "        best_author = authors_list[best_idx]\n",
    "        best_score = similarity_scores[best_idx].item()\n",
    "        \n",
    "        return best_author, best_score\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "print(\"Elastic transform prediction function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac5aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results from CSV and filter for correct predictions\n",
    "correct_predictions_df = pd.read_csv(\"top100_results_df.csv\")\n",
    "correct_predictions_df = correct_predictions_df[correct_predictions_df['correct'] == True]\n",
    "\n",
    "# Convert to list of dictionaries for compatibility with existing code\n",
    "correct_predictions = correct_predictions_df.to_dict('records')\n",
    "\n",
    "print(f\"Total correct predictions loaded from CSV: {len(correct_predictions)}\")\n",
    "print(f\"\\nFirst 5 correct predictions:\")\n",
    "for i, pred in enumerate(correct_predictions[:5], 1):\n",
    "    print(f\"{i}. {pred['title']} by {pred['true_author']} (confidence: {pred['confidence']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc33400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-predict authors on elastic-transformed versions of correctly predicted images\n",
    "elastic_results = []\n",
    "\n",
    "print(f\"Re-predicting authors on {len(correct_predictions)} elastic-transformed images...\\n\")\n",
    "print(f\"Using ElasticTransform with alpha=250.0, sigma=5.0\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "total = len(correct_predictions)\n",
    "for counter, result in enumerate(correct_predictions, 1):\n",
    "    \n",
    "    image_file = result['image_file']\n",
    "    true_author = result['true_author']\n",
    "    original_prediction = result['predicted_author']\n",
    "    original_confidence = result['confidence']\n",
    "    title = result['title']\n",
    "    image_path = os.path.join(\"Images\", image_file)\n",
    "    \n",
    "    # Show progress every 200 paintings or for first 10\n",
    "    if counter <= 10 or counter % 200 == 0:\n",
    "        print(f\"\\n[{counter}/{total}] Processing: {title} by {true_author}\")\n",
    "    \n",
    "    # Predict author on elastic-transformed image\n",
    "    predicted_author, confidence = predict_author_for_elastic_image(\n",
    "        image_path, top_100_authors, model, processor, device, alpha=250.0, sigma=5.0\n",
    "    )\n",
    "    \n",
    "    if predicted_author:\n",
    "        is_correct = predicted_author == true_author\n",
    "        symbol = \"✅\" if is_correct else \"❌\"\n",
    "        \n",
    "        # Only show detailed predictions for first 10 or every 200th painting\n",
    "        if counter <= 10 or counter % 200 == 0:\n",
    "            print(f\"   Original (no transform): ✅ {original_prediction} (confidence: {original_confidence:.4f})\")\n",
    "            print(f\"   Elastic transform: {symbol} {predicted_author} (confidence: {confidence:.4f})\")\n",
    "        \n",
    "        elastic_results.append({\n",
    "            'image_file': image_file,\n",
    "            'title': title,\n",
    "            'true_author': true_author,\n",
    "            'original_prediction': original_prediction,\n",
    "            'original_confidence': original_confidence,\n",
    "            'elastic_prediction': predicted_author,\n",
    "            'elastic_confidence': confidence,\n",
    "            'still_correct': is_correct\n",
    "        })\n",
    "\n",
    "print(f\"\\n\\nElastic transform re-prediction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e6526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison: Original vs Elastic Transform\n",
    "if elastic_results:\n",
    "    elastic_results_df = pd.DataFrame(elastic_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"ELASTIC TRANSFORM vs ORIGINAL PREDICTION COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Calculate accuracy on elastic-transformed images\n",
    "    elastic_accuracy = elastic_results_df['still_correct'].sum() / len(elastic_results_df) * 100\n",
    "    \n",
    "    print(f\"\\nOriginal Images (no transformation):\")\n",
    "    print(f\"  Total paintings tested: {len(elastic_results_df)}\")\n",
    "    print(f\"  All were correctly predicted: 100% (these were the correct predictions from original test)\")\n",
    "    \n",
    "    print(f\"\\nElastic-Transformed Images (alpha=250.0, sigma=5.0):\")\n",
    "    print(f\"  Total paintings tested: {len(elastic_results_df)}\")\n",
    "    print(f\"  Still correctly predicted: {elastic_results_df['still_correct'].sum()}\")\n",
    "    print(f\"  Accuracy: {elastic_accuracy:.2f}%\")\n",
    "    \n",
    "    # Calculate drop in accuracy\n",
    "    accuracy_drop = 100 - elastic_accuracy\n",
    "    print(f\"\\nAccuracy Drop: {accuracy_drop:.2f}%\")\n",
    "    print(f\"  ({elastic_results_df['still_correct'].sum()}/{len(elastic_results_df)} paintings remained correct after elastic transform)\")\n",
    "    \n",
    "    # Confidence comparison\n",
    "    print(f\"\\nConfidence Statistics:\")\n",
    "    print(f\"  Original (no transform) - Average: {elastic_results_df['original_confidence'].mean():.4f}\")\n",
    "    print(f\"  Elastic transform - Average: {elastic_results_df['elastic_confidence'].mean():.4f}\")\n",
    "    \n",
    "    confidence_diff = elastic_results_df['elastic_confidence'].mean() - elastic_results_df['original_confidence'].mean()\n",
    "    print(f\"  Confidence change: {confidence_diff:+.4f}\")\n",
    "    \n",
    "    # Compare with grayscale results if available\n",
    "    if 'grayscale_results_df' in globals():\n",
    "        print(f\"\\n\\nComparison with Grayscale Transformation:\")\n",
    "        print(f\"  Grayscale accuracy: {(grayscale_results_df['still_correct'].sum() / len(grayscale_results_df) * 100):.2f}%\")\n",
    "        print(f\"  Elastic accuracy: {elastic_accuracy:.2f}%\")\n",
    "        elastic_vs_gray = elastic_accuracy - (grayscale_results_df['still_correct'].sum() / len(grayscale_results_df) * 100)\n",
    "        print(f\"  Elastic is {elastic_vs_gray:+.2f}% {'better' if elastic_vs_gray > 0 else 'worse'} than grayscale\")\n",
    "    \n",
    "    # Analyze paintings that became incorrect\n",
    "    became_incorrect_elastic = elastic_results_df[~elastic_results_df['still_correct']]\n",
    "    \n",
    "    if len(became_incorrect_elastic) > 0:\n",
    "        print(f\"\\n\\nPaintings that became INCORRECT after elastic transform ({len(became_incorrect_elastic)} total):\")\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "        print(f\"\\nMost common new predictions (what model confused them with):\")\n",
    "        new_predictions_elastic = became_incorrect_elastic['elastic_prediction'].value_counts().head(10)\n",
    "        for author, count in new_predictions_elastic.items():\n",
    "            print(f\"  {author}: {count} paintings\")\n",
    "        \n",
    "        print(f\"\\nAuthors most affected by elastic transform (lost correct predictions):\")\n",
    "        affected_authors_elastic = became_incorrect_elastic['true_author'].value_counts().head(10)\n",
    "        for author, count in affected_authors_elastic.items():\n",
    "            total_by_author = len(elastic_results_df[elastic_results_df['true_author'] == author])\n",
    "            print(f\"  {author}: {count}/{total_by_author} became incorrect ({count/total_by_author*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nSample of paintings that lost correct prediction:\")\n",
    "        display(became_incorrect_elastic[['title', 'true_author', 'original_prediction', 'elastic_prediction', \n",
    "                                           'original_confidence', 'elastic_confidence']].head(20))\n",
    "    \n",
    "    # Paintings that remained correct\n",
    "    remained_correct_elastic = elastic_results_df[elastic_results_df['still_correct']]\n",
    "    \n",
    "    if len(remained_correct_elastic) > 0:\n",
    "        print(f\"\\n\\nPaintings that remained CORRECT after elastic transform ({len(remained_correct_elastic)} total):\")\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "        print(f\"\\nAuthors most robust to elastic transform:\")\n",
    "        robust_authors_elastic = remained_correct_elastic['true_author'].value_counts().head(10)\n",
    "        for author, count in robust_authors_elastic.items():\n",
    "            total_by_author = len(elastic_results_df[elastic_results_df['true_author'] == author])\n",
    "            print(f\"  {author}: {count}/{total_by_author} remained correct ({count/total_by_author*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nSample of paintings that remained correct:\")\n",
    "        display(remained_correct_elastic[['title', 'true_author', 'original_confidence', 'elastic_confidence']].head(20))\n",
    "    \n",
    "    # Side-by-side comparison of transformations\n",
    "    if 'grayscale_results_df' in globals():\n",
    "        print(f\"\\n\\nSide-by-Side Comparison: Grayscale vs Elastic Transform\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        # Find paintings that failed in both\n",
    "        gray_failed = set(grayscale_results_df[~grayscale_results_df['still_correct']]['image_file'])\n",
    "        elastic_failed = set(elastic_results_df[~elastic_results_df['still_correct']]['image_file'])\n",
    "        \n",
    "        both_failed = gray_failed & elastic_failed\n",
    "        only_gray_failed = gray_failed - elastic_failed\n",
    "        only_elastic_failed = elastic_failed - gray_failed\n",
    "        \n",
    "        print(f\"\\nFailure patterns:\")\n",
    "        print(f\"  Failed in BOTH transformations: {len(both_failed)} paintings\")\n",
    "        print(f\"  Failed ONLY in grayscale: {len(only_gray_failed)} paintings\")\n",
    "        print(f\"  Failed ONLY in elastic: {len(only_elastic_failed)} paintings\")\n",
    "        print(f\"\\nInterpretation:\")\n",
    "        print(f\"  - Paintings that fail in both are likely highly sensitive to any distortion\")\n",
    "        print(f\"  - Paintings that fail only in grayscale rely heavily on color information\")\n",
    "        print(f\"  - Paintings that fail only in elastic are sensitive to geometric distortions but not color\")\n",
    "    \n",
    "    print(\"\\n\\nAll Results (first 50 rows):\")\n",
    "    display(elastic_results_df[['title', 'true_author', 'original_prediction', 'elastic_prediction', \n",
    "                                 'original_confidence', 'elastic_confidence', 'still_correct']].head(50))\n",
    "\n",
    "else:\n",
    "    print(\"No results to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76367f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_results_df.to_csv(\"grayscale_results_df.csv\", index=False)\n",
    "perspective_results_df.to_csv(\"perspective_results_df.csv\", index=False)\n",
    "elastic_results_df.to_csv(\"elastic_results_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
